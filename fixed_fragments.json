{
  "fragments": [
    {
      "uid": "7f320289e2332da0",
      "name": "main",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def main():\n    \"\"\"Main CLI entry point\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Autopsy Pro - Extract and rebuild code from inactive projects'\n    )\n    \n    subparsers = parser.add_subparsers(dest='command', help='Commands')\n    \n    # Scan command\n    scan_parser = subparsers.add_parser('scan', help='Scan for projects')\n    scan_parser.add_argument('directory', help='Directory to scan')\n    scan_parser.add_argument('--inactive-days', type=int, help='Days of inactivity threshold')\n    scan_parser.add_argument('--include-active', action='store_true', help='Include active projects')\n    scan_parser.add_argument('--extensions', help='Comma-separated file extensions')\n    scan_parser.add_argument('--no-cache', action='store_true', help='Disable cache')\n    scan_parser.add_argument('-o', '--output', help='Save results to file')\n    scan_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n    \n    # Extract command\n    extract_parser = subparsers.add_parser('extract', help='Extract code fragments')\n    extract_parser.add_argument('--directory', help='Directory to scan (if no scan file)')\n    extract_parser.add_argument('--scan-file', help='Use existing scan results')\n    extract_parser.add_argument('--min-quality', type=int, help='Minimum quality score')\n    extract_parser.add_argument('--skip-tests', action='store_true', help='Skip test fragments')\n    extract_parser.add_argument('--no-dedupe', action='store_true', help='Disable deduplication')\n    extract_parser.add_argument('-o', '--output', help='Save results to file')\n    extract_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n    \n    # Export command\n    export_parser = subparsers.add_parser('export', help='Export fragment collection')\n    export_parser.add_argument('--fragments-file', required=True, help='Fragments JSON file')\n    export_parser.add_argument('--name', help='Export name')\n    export_parser.add_argument('--description', help='Export description')\n    export_parser.add_argument('--min-quality', type=int, help='Filter by minimum quality')\n    export_parser.add_argument('--language', help='Filter by language')\n    \n    # Import command\n    import_parser = subparsers.add_parser('import', help='Import fragment collection')\n    import_parser.add_argument('file', help='Import file')\n    import_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')\n    \n    # Config command\n    config_parser = subparsers.add_parser('config', help='Manage configuration')\n    config_parser.add_argument('--show', action='store_true', help='Show current config')\n    config_parser.add_argument('--reset', action='store_true', help='Reset to defaults')\n    config_parser.add_argument('--set', help='Set config value (key=value)')\n    \n    # Cache command\n    cache_parser = subparsers.add_parser('cache', help='Manage cache')\n    cache_parser.add_argument('--clear', action='store_true', help='Clear cache')\n    \n    args = parser.parse_args()\n    \n    if not args.command:\n        parser.print_help()\n        return 1\n    \n    # Route to command handler\n    handlers = {\n        'scan': cmd_scan,\n        'extract': cmd_extract,\n        'export': cmd_export,\n        'import': cmd_import,\n        'config': cmd_config,\n        'cache': cmd_cache,\n    }\n    \n    try:\n        return handlers[args.command](args)\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted\")\n        return 130\n    except Exception as e:\n        logger.exception(\"Error executing command\")\n        print(f\"Error: {e}\")\n        return 1",
      "lines": 76,
      "quality": 10,
      "start_line": 245,
      "end_line": 320,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 7,
      "documentation_ratio": 0.12307692307692308,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "a41519ae596e0c9c80e2b6bf7e1abaf0",
      "similar_fragments": []
    },
    {
      "uid": "cbdebcebf340972e",
      "name": "extract_js_fragments",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_js_fragments(code: str, file_path: Path, project_name: str, config: Config) -> List[Fragment]:\n    \"\"\"Extract JavaScript/TypeScript fragments\"\"\"\n    fragments = []\n    lines = code.splitlines()\n    \n    patterns = [\n        (r\"(?:export\\s+)?(?:async\\s+)?function\\s+(\\w+)\", \"Function\"),\n        (r\"(?:export\\s+)?class\\s+(\\w+)\", \"Class\"),\n        (r\"const\\s+(\\w+)\\s*=\\s*(?:async\\s*)?\\([^)]*\\)\\s*=>\", \"ArrowFunc\"),\n        (r\"(?:export\\s+)?(?:const|let|var)\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>\", \"Component\"),\n    ]\n    \n    for pattern, frag_type in patterns:\n        for match in re.finditer(pattern, code):\n            name = match.group(1)\n            start_line = code[:match.start()].count(\"\\n\")\n            \n            block, end_line = extract_js_block(lines, start_line)\n            \n            # Skip if too short or too long\n            block_lines = block.splitlines()\n            if len(block_lines) < config.min_lines or len(block_lines) > config.max_lines:\n                continue\n            \n            # Quality assessment\n            quality, metrics = assess_quality_enhanced(block, 'javascript', frag_type)\n            \n            if quality < config.min_quality:\n                continue\n            \n            # Extract imports\n            imports = re.findall(r'import\\s+.*?from\\s+[\\'\"]([^\\'\"]+)[\\'\"]', block)\n            \n            # Check for exports\n            exports = []\n            if 'export' in block:\n                exports.append(name)\n            \n            uid = stable_uid(project_name, file_path.name, str(start_line))\n            \n            fragment = Fragment(\n                uid=uid,\n                name=name,\n                type=f\"JS/{frag_type}\",\n                file=str(file_path),\n                project=project_name,\n                code=block,\n                lines=len(block_lines),\n                quality=quality,\n                start_line=start_line + 1,\n                end_line=end_line + 1,\n                imports=imports,\n                exports=exports,\n                complexity=metrics.get('complexity', 0),\n                documentation_ratio=metrics.get('doc_ratio', 0.0),\n                has_types=metrics.get('has_types', False),\n                has_error_handling=metrics.get('has_error_handling', False)\n            )\n            \n            fragments.append(fragment)\n    \n    return fragments",
      "lines": 62,
      "quality": 10,
      "start_line": 313,
      "end_line": 374,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 10,
      "documentation_ratio": 0.1,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "5b015d3b87f6c1031a62cdbb72f2c53a",
      "similar_fragments": []
    },
    {
      "uid": "eef5fd9d2d584555",
      "name": "Project",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "class Project:\n    \"\"\"Project metadata\"\"\"\n    name: str\n    path: str\n    type: str\n    files: int\n    size_bytes: int\n    last_modified: float\n    code_files: List[str]\n    \n    # Enhanced metadata\n    dependencies: List[str] = field(default_factory=list)\n    frameworks: List[str] = field(default_factory=list)\n    languages: Set[str] = field(default_factory=set)\n    complexity_score: float = 0.0\n    health_score: float = 0.0\n    \n    @property\n    def size_mb(self) -> float:\n        \"\"\"Size in megabytes\"\"\"\n        return self.size_bytes / (1024 * 1024)\n    \n    @property\n    def last_modified_date(self) -> datetime:\n        \"\"\"Last modified as datetime\"\"\"\n        return datetime.fromtimestamp(self.last_modified)\n    \n    @property\n    def days_inactive(self) -> int:\n        \"\"\"Days since last modification\"\"\"\n        return (datetime.now() - self.last_modified_date).days\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization\"\"\"\n        data = asdict(self)\n        data['languages'] = list(self.languages)  # Convert set to list\n        return data\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Project':\n        \"\"\"Create from dictionary\"\"\"\n        if 'languages' in data and isinstance(data['languages'], list):\n            data['languages'] = set(data['languages'])\n        return cls(**data)",
      "lines": 44,
      "quality": 10,
      "start_line": 10,
      "end_line": 53,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.18421052631578946,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "c0e3edb93affa3eaf74981e373960248",
      "similar_fragments": []
    },
    {
      "uid": "38e89575853c79c7",
      "name": "ScanResult",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "class ScanResult:\n    \"\"\"Result of a project scan\"\"\"\n    base_path: str\n    projects: List[Project]\n    scan_time: float\n    timestamp: datetime = field(default_factory=datetime.now)\n    \n    @property\n    def total_files(self) -> int:\n        return sum(p.files for p in self.projects)\n    \n    @property\n    def total_code_files(self) -> int:\n        return sum(len(p.code_files) for p in self.projects)\n    \n    @property\n    def total_size_mb(self) -> float:\n        return sum(p.size_mb for p in self.projects)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'base_path': self.base_path,\n            'projects': [p.to_dict() for p in self.projects],\n            'scan_time': self.scan_time,\n            'timestamp': self.timestamp.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ScanResult':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            base_path=data['base_path'],\n            projects=[Project.from_dict(p) for p in data['projects']],\n            scan_time=data['scan_time'],\n            timestamp=datetime.fromisoformat(data['timestamp'])\n        )",
      "lines": 37,
      "quality": 10,
      "start_line": 139,
      "end_line": 175,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 6,
      "documentation_ratio": 0.09375,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "071b11073f6b5a56cc496c2965f7195e",
      "similar_fragments": []
    },
    {
      "uid": "db74f639af4c67f5",
      "name": "ExtractionResult",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "class ExtractionResult:\n    \"\"\"Result of fragment extraction\"\"\"\n    fragments: List[Fragment]\n    extraction_time: float\n    timestamp: datetime = field(default_factory=datetime.now)\n    \n    @property\n    def total_lines(self) -> int:\n        return sum(f.lines for f in self.fragments)\n    \n    @property\n    def avg_quality(self) -> float:\n        if not self.fragments:\n            return 0.0\n        return sum(f.quality for f in self.fragments) / len(self.fragments)\n    \n    @property\n    def languages(self) -> Set[str]:\n        return {f.language for f in self.fragments}\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'fragments': [f.to_dict() for f in self.fragments],\n            'extraction_time': self.extraction_time,\n            'timestamp': self.timestamp.isoformat()\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractionResult':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            fragments=[Fragment.from_dict(f) for f in data['fragments']],\n            extraction_time=data['extraction_time'],\n            timestamp=datetime.fromisoformat(data['timestamp'])\n        )",
      "lines": 36,
      "quality": 10,
      "start_line": 179,
      "end_line": 214,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 7,
      "documentation_ratio": 0.0967741935483871,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "086e34cadef729fe0a9a84aeaa4b17ad",
      "similar_fragments": []
    },
    {
      "uid": "637e9ab270b5be86",
      "name": "deduplicate_fragments",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def deduplicate_fragments(fragments: List[Fragment]) -> List[Fragment]:\n    \"\"\"\n    Remove duplicate fragments using both exact and semantic matching\n    \"\"\"\n    seen_exact = set()\n    seen_semantic = set()\n    unique = []\n    \n    for frag in fragments:\n        # Check exact match\n        if frag.code_hash in seen_exact:\n            logger.debug(f\"Skipping exact duplicate: {frag.name}\")\n            continue\n        \n        # Compute semantic hash\n        semantic_hash = frag.compute_embedding_hash()\n        \n        # Check semantic similarity (simple version)\n        if semantic_hash in seen_semantic:\n            logger.debug(f\"Skipping semantic duplicate: {frag.name}\")\n            continue\n        \n        seen_exact.add(frag.code_hash)\n        seen_semantic.add(semantic_hash)\n        unique.append(frag)\n    \n    logger.info(f\"Deduplicated: {len(fragments)} -> {len(unique)} fragments\")\n    return unique",
      "lines": 28,
      "quality": 10,
      "start_line": 398,
      "end_line": 425,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 5,
      "documentation_ratio": 0.21739130434782608,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "c43a2a24e03c00460a3bf211df047d17",
      "similar_fragments": []
    },
    {
      "uid": "63eebe40e5b03241",
      "name": "load_config",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def load_config() -> Config:\n    \"\"\"Load configuration from file or create default\"\"\"\n    ensure_dirs()\n    \n    if CONFIG_FILE.exists():\n        try:\n            with open(CONFIG_FILE, 'r') as f:\n                data = json.load(f)\n            config = Config.from_dict(data)\n            \n            # Validate\n            issues = config.validate()\n            if issues:\n                logger.warning(f\"Config validation issues: {issues}\")\n                logger.warning(\"Using defaults for invalid values\")\n                return Config()\n            \n            return config\n        except Exception as e:\n            logger.error(f\"Error loading config: {e}\")\n            logger.info(\"Using default configuration\")\n            return Config()\n    else:\n        config = Config()\n        save_config(config)\n        return config",
      "lines": 26,
      "quality": 10,
      "start_line": 102,
      "end_line": 127,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 7,
      "documentation_ratio": 0.08695652173913043,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "0712bec205e3cc38d1f85c4808116b27",
      "similar_fragments": []
    },
    {
      "uid": "bac78256147ff65e",
      "name": "is_project_root",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/scanner.py",
      "project": "autopsy_pro_v3",
      "code": "def is_project_root(path: Path, files: List[str]) -> bool:\n    \"\"\"Determine if directory is a project root\"\"\"\n    # Check for common project indicators\n    indicators = set(PROJECT_INDICATORS.keys())\n    \n    if any(f in indicators for f in files):\n        return True\n    \n    # Check for common project structure\n    dir_name = path.name.lower()\n    \n    # Common project patterns\n    project_patterns = ['project', 'app', 'service', 'api', 'web', 'backend', 'frontend', 'server']\n    if any(pattern in dir_name for pattern in project_patterns):\n        try:\n            subdirs = [d for d in path.iterdir() if d.is_dir()]\n            subdir_names = {d.name.lower() for d in subdirs}\n            \n            # Check for source directories\n            src_indicators = ['src', 'lib', 'source', 'app', 'components', 'pages', 'api', 'handlers']\n            if any(name in subdir_names for name in src_indicators):\n                return True\n        except PermissionError:\n            pass\n    \n    return False",
      "lines": 26,
      "quality": 10,
      "start_line": 123,
      "end_line": 148,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 15,
      "documentation_ratio": 0.23809523809523808,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "eb4c2112f13b06fcb29b28f1ba224821",
      "similar_fragments": []
    },
    {
      "uid": "f59176b34c5e9f10",
      "name": "BuildResult",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "class BuildResult:\n    \"\"\"Result of project build\"\"\"\n    success: bool\n    output_path: str\n    fragments_used: int\n    errors: List[str] = field(default_factory=list)\n    warnings: List[str] = field(default_factory=list)\n    build_time: float = 0.0\n    timestamp: datetime = field(default_factory=datetime.now)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'success': self.success,\n            'output_path': self.output_path,\n            'fragments_used': self.fragments_used,\n            'errors': self.errors,\n            'warnings': self.warnings,\n            'build_time': self.build_time,\n            'timestamp': self.timestamp.isoformat()\n        }",
      "lines": 21,
      "quality": 10,
      "start_line": 218,
      "end_line": 238,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 1,
      "documentation_ratio": 0.1,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "2db26e0c0da8780a1da20b022c26f20a",
      "similar_fragments": []
    },
    {
      "uid": "793fc13d749f0f53",
      "name": "validate",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "    def validate(self) -> List[str]:\n        \"\"\"Validate configuration, return list of issues\"\"\"\n        issues = []\n        \n        if self.inactive_days < 0:\n            issues.append(\"inactive_days must be >= 0\")\n        if self.max_file_mb <= 0:\n            issues.append(\"max_file_mb must be > 0\")\n        if not 1 <= self.min_quality <= 10:\n            issues.append(\"min_quality must be between 1-10\")\n        if self.min_lines < 1:\n            issues.append(\"min_lines must be >= 1\")\n        if self.max_lines < self.min_lines:\n            issues.append(\"max_lines must be >= min_lines\")\n        if self.max_workers < 1:\n            issues.append(\"max_workers must be >= 1\")\n        if self.cache_ttl_hours < 1:\n            issues.append(\"cache_ttl_hours must be >= 1\")\n        \n        return issues",
      "lines": 20,
      "quality": 10,
      "start_line": 73,
      "end_line": 92,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 8,
      "documentation_ratio": 0.05555555555555555,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "0a06feee48464889c0a3a7bb68971696",
      "similar_fragments": []
    },
    {
      "uid": "3d7ba095e48fdd99",
      "name": "extract_fragments",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_fragments(projects: List[Project], config: Config) -> ExtractionResult:\n    \"\"\"\n    Main entry point for fragment extraction\n    \"\"\"\n    start_time = time.time()\n    \n    fragments = extract_fragments_parallel(projects, config)\n    \n    # Sort by quality (highest first)\n    fragments.sort(key=lambda f: (f.quality, f.lines), reverse=True)\n    \n    extraction_time = time.time() - start_time\n    \n    logger.info(f\"Extraction complete: {len(fragments)} fragments in {extraction_time:.2f}s\")\n    logger.info(f\"Average quality: {sum(f.quality for f in fragments) / len(fragments):.1f}\" if fragments else \"N/A\")\n    \n    return ExtractionResult(\n        fragments=fragments,\n        extraction_time=extraction_time\n    )",
      "lines": 20,
      "quality": 10,
      "start_line": 474,
      "end_line": 493,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 5,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "86992cf7594c86d475befaee9916eb10",
      "similar_fragments": []
    },
    {
      "uid": "cdfee2d25d070908",
      "name": "extract_from_file",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_from_file(file_path: Path, project: Project, config: Config) -> List[Fragment]:\n    \"\"\"Extract fragments from a single file\"\"\"\n    try:\n        code = safe_read_text(file_path)\n        if not code:\n            return []\n        \n        # Determine language and extract\n        if file_path.suffix == '.py':\n            return extract_python_fragments(code, file_path, project.name, config)\n        elif file_path.suffix in ['.js', '.jsx', '.ts', '.tsx']:\n            return extract_js_fragments(code, file_path, project.name, config)\n        # Add more languages as needed (Go, Rust, Java, etc.)\n        \n        return []\n    \n    except Exception as e:\n        logger.error(f\"Error extracting from {file_path}: {e}\")\n        return []",
      "lines": 19,
      "quality": 10,
      "start_line": 377,
      "end_line": 395,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 6,
      "documentation_ratio": 0.1875,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "a92bcc77d24eb8b12e59ae1801673747",
      "similar_fragments": []
    },
    {
      "uid": "ea5c87b5272bd3f5",
      "name": "save_config",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def save_config(config: Config) -> bool:\n    \"\"\"Save configuration to file\"\"\"\n    ensure_dirs()\n    \n    # Validate before saving\n    issues = config.validate()\n    if issues:\n        logger.error(f\"Cannot save invalid config: {issues}\")\n        return False\n    \n    try:\n        with open(CONFIG_FILE, 'w') as f:\n            json.dump(config.to_dict(), f, indent=2)\n        logger.info(f\"Configuration saved to {CONFIG_FILE}\")\n        return True\n    except Exception as e:\n        logger.error(f\"Error saving config: {e}\")\n        return False",
      "lines": 18,
      "quality": 10,
      "start_line": 130,
      "end_line": 147,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 3,
      "documentation_ratio": 0.125,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "5dcee6b83e10a0c847f0b07874fc08a8",
      "similar_fragments": []
    },
    {
      "uid": "f2c86688dfbc944a",
      "name": "safe_read_text",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def safe_read_text(path: Path, encoding: str = 'utf-8') -> Optional[str]:\n    \"\"\"\n    Safely read text file with fallback encodings\n    \"\"\"\n    encodings = [encoding, 'utf-8', 'latin-1', 'cp1252']\n    \n    for enc in encodings:\n        try:\n            with open(path, 'r', encoding=enc) as f:\n                return f.read()\n        except UnicodeDecodeError:\n            continue\n        except Exception as e:\n            logger.error(f\"Error reading {path}: {e}\")\n            return None\n    \n    logger.warning(f\"Could not decode {path} with any encoding\")\n    return None",
      "lines": 18,
      "quality": 10,
      "start_line": 10,
      "end_line": 27,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.125,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "0352c91e6abc2d21ad5878540a2f1366",
      "similar_fragments": []
    },
    {
      "uid": "2ea5fa0d37cb475a",
      "name": "sanitize_filename",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def sanitize_filename(name: str) -> str:\n    \"\"\"Sanitize a string for use as filename\"\"\"\n    # Replace invalid characters\n    invalid_chars = '<>:\"/\\\\|?*'\n    for char in invalid_chars:\n        name = name.replace(char, '_')\n    \n    # Limit length\n    if len(name) > 200:\n        name = name[:200]\n    \n    return name.strip()",
      "lines": 12,
      "quality": 10,
      "start_line": 59,
      "end_line": 70,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.3,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "76e060aa547dd03c2dd685ddc5d74554",
      "similar_fragments": []
    },
    {
      "uid": "968c55d9e48b634e",
      "name": "to_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'success': self.success,\n            'output_path': self.output_path,\n            'fragments_used': self.fragments_used,\n            'errors': self.errors,\n            'warnings': self.warnings,\n            'build_time': self.build_time,\n            'timestamp': self.timestamp.isoformat()\n        }",
      "lines": 11,
      "quality": 10,
      "start_line": 228,
      "end_line": 238,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 1,
      "documentation_ratio": 0.09090909090909091,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "74568e7477211dd10be32d759a22de90",
      "similar_fragments": []
    },
    {
      "uid": "10844f7c48da47e6",
      "name": "compute_embedding_hash",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def compute_embedding_hash(self) -> str:\n        \"\"\"Compute a simple semantic hash (would use embeddings in production)\"\"\"\n        # Normalize code for semantic comparison\n        normalized = self.code.lower()\n        normalized = ''.join(c if c.isalnum() else ' ' for c in normalized)\n        words = normalized.split()\n        word_set = sorted(set(words))\n        semantic_text = ' '.join(word_set[:50])  # First 50 unique words\n        self.embedding_hash = hashlib.md5(semantic_text.encode()).hexdigest()\n        return self.embedding_hash",
      "lines": 10,
      "quality": 10,
      "start_line": 117,
      "end_line": 126,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 5,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "597646c65b0fdc027c6ee0a48c51ab69",
      "similar_fragments": []
    },
    {
      "uid": "636f9dc1b1310b6c",
      "name": "format_duration",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def format_duration(seconds: float) -> str:\n    \"\"\"Format duration in seconds to human readable\"\"\"\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        minutes = seconds / 60\n        return f\"{minutes:.1f}m\"\n    else:\n        hours = seconds / 3600\n        return f\"{hours:.1f}h\"",
      "lines": 10,
      "quality": 10,
      "start_line": 47,
      "end_line": 56,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.1,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "fd10624a27d27ee707a3c672bfd60cd3",
      "similar_fragments": []
    },
    {
      "uid": "d7090e6f9a102a40",
      "name": "clear_cache",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def clear_cache():\n    \"\"\"Clear all cached data\"\"\"\n    if CACHE_DIR.exists():\n        for cache_file in CACHE_DIR.glob(\"*.json\"):\n            try:\n                cache_file.unlink()\n            except Exception as e:\n                logger.error(f\"Error deleting cache file {cache_file}: {e}\")\n        logger.info(\"Cache cleared\")",
      "lines": 9,
      "quality": 10,
      "start_line": 165,
      "end_line": 173,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.1111111111111111,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "d259e3fe917a6d530a793ce12d6a14aa",
      "similar_fragments": []
    },
    {
      "uid": "9538eba22b1b5831",
      "name": "get_cache_path",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def get_cache_path(key: str) -> Path:\n    \"\"\"Get cache file path for a key\"\"\"\n    ensure_dirs()\n    # Sanitize key for filename\n    safe_key = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in key)\n    return CACHE_DIR / f\"{safe_key}.json\"",
      "lines": 6,
      "quality": 10,
      "start_line": 150,
      "end_line": 155,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 7,
      "documentation_ratio": 0.3333333333333333,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "4ac4c12b6e73a00c51e9c9e1be224396",
      "similar_fragments": []
    },
    {
      "uid": "4c5f5ba0438f0f5e",
      "name": "from_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def from_dict(cls, data: Dict[str, Any]) -> 'Project':\n        \"\"\"Create from dictionary\"\"\"\n        if 'languages' in data and isinstance(data['languages'], list):\n            data['languages'] = set(data['languages'])\n        return cls(**data)",
      "lines": 5,
      "quality": 10,
      "start_line": 49,
      "end_line": 53,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 3,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "e870023ea85ee3e8c1fcfcc1edbae0ef",
      "similar_fragments": []
    },
    {
      "uid": "386780dfbae8bfaa",
      "name": "get_export_path",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def get_export_path(name: str) -> Path:\n    \"\"\"Get export file path\"\"\"\n    ensure_dirs()\n    safe_name = \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in name)\n    return EXPORT_DIR / f\"{safe_name}.json\"",
      "lines": 5,
      "quality": 10,
      "start_line": 158,
      "end_line": 162,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 5,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "ba8978b5f78027e913ae051b3ebbed8c",
      "similar_fragments": []
    },
    {
      "uid": "aa36477c31902fcc",
      "name": "cmd_scan",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_scan(args):\n    \"\"\"Scan for projects\"\"\"\n    config = load_config()\n    \n    # Override config with CLI args\n    if args.inactive_days is not None:\n        config.inactive_days = args.inactive_days\n    if args.include_active:\n        config.include_active = True\n    if args.extensions:\n        config.exts = args.extensions.split(',')\n    \n    base_path = Path(args.directory).expanduser().resolve()\n    \n    if not base_path.exists():\n        print(f\"Error: Directory not found: {base_path}\")\n        return 1\n    \n    print(f\"Scanning {base_path}...\")\n    result = scan_projects(base_path, config, use_cache=not args.no_cache)\n    \n    print(f\"\\nScan Results:\")\n    print(f\"  Projects found: {len(result.projects)}\")\n    print(f\"  Total files: {result.total_files:,}\")\n    print(f\"  Code files: {result.total_code_files:,}\")\n    print(f\"  Total size: {result.total_size_mb:.1f} MB\")\n    print(f\"  Scan time: {result.scan_time:.2f}s\")\n    \n    if args.verbose:\n        print(f\"\\nProjects:\")\n        for i, p in enumerate(result.projects[:20], 1):\n            print(f\"  {i}. {p.name} ({p.type}) - {len(p.code_files)} files, {p.days_inactive} days old\")\n    \n    # Save if requested\n    if args.output:\n        output_path = Path(args.output)\n        with open(output_path, 'w') as f:\n            json.dump(result.to_dict(), f, indent=2)\n        print(f\"\\nResults saved to: {output_path}\")\n    \n    return 0",
      "lines": 41,
      "quality": 9,
      "start_line": 22,
      "end_line": 62,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 10,
      "documentation_ratio": 0.09090909090909091,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "ba80cdd08474b44331b80943ad225551",
      "similar_fragments": []
    },
    {
      "uid": "85375f330a8fb899",
      "name": "scan_projects",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/scanner.py",
      "project": "autopsy_pro_v3",
      "code": "def scan_projects(base: Path, config: Config, use_cache: bool = True) -> ScanResult:\n    \"\"\"\n    Main entry point for project scanning with optional caching\n    \"\"\"\n    cache_key = f\"scan_{base}_{config.inactive_days}_{config.include_active}\"\n    cache_file = get_cache_path(cache_key)\n    \n    # Try to load from cache\n    if use_cache and config.enable_cache and cache_file.exists():\n        try:\n            cache_age = time.time() - cache_file.stat().st_mtime\n            if cache_age < config.cache_ttl_hours * 3600:\n                with open(cache_file, 'r') as f:\n                    data = json.load(f)\n                result = ScanResult.from_dict(data)\n                logger.info(f\"Loaded scan results from cache ({len(result.projects)} projects)\")\n                return result\n        except Exception as e:\n            logger.warning(f\"Error loading cache: {e}\")\n    \n    # Perform scan\n    start_time = time.time()\n    projects = scan_projects_parallel(base, config)\n    scan_time = time.time() - start_time\n    \n    result = ScanResult(\n        base_path=str(base),\n        projects=projects,\n        scan_time=scan_time\n    )\n    \n    # Save to cache\n    if config.enable_cache:\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(result.to_dict(), f)\n            logger.info(\"Scan results cached\")\n        except Exception as e:\n            logger.warning(f\"Error saving cache: {e}\")\n    \n    return result",
      "lines": 41,
      "quality": 9,
      "start_line": 336,
      "end_line": 376,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 9,
      "documentation_ratio": 0.1388888888888889,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "f8d83f03a83fad40eb3a0817d79232ff",
      "similar_fragments": []
    },
    {
      "uid": "e0daa7823ddd2cb2",
      "name": "compute_complexity",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def compute_complexity(code: str, lang: str) -> int:\n    \"\"\"\n    Compute cyclomatic complexity estimate\n    \"\"\"\n    complexity = 1  # Base complexity\n\n    # Decision points - separate keywords from operators\n    python_keywords = ['if', 'elif', 'else', 'for', 'while', 'except']\n    other_keywords = ['case', 'catch']\n    operators = ['&&', '||', '?']\n\n    if lang in ['py', 'python']:\n        # Use word boundaries for Python keywords only\n        for keyword in python_keywords:\n            complexity += len(re.findall(rf'\\b{re.escape(keyword)}\\b', code))\n        # Python uses 'and', 'or' instead of &&, ||\n        complexity += len(re.findall(r'\\b(and|or)\\b', code))\n    else:\n        # For other languages, use word boundaries for keywords\n        for keyword in python_keywords + other_keywords:\n            complexity += len(re.findall(rf'\\b{re.escape(keyword)}\\b', code))\n        # Count operators separately without word boundaries\n        for operator in operators:\n            complexity += code.count(operator)\n\n    return complexity",
      "lines": 26,
      "quality": 9,
      "start_line": 17,
      "end_line": 42,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 18,
      "documentation_ratio": 0.30434782608695654,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "228e2eeb25269d46df1e11fa8eb181fe",
      "similar_fragments": []
    },
    {
      "uid": "013b21562ca3753e",
      "name": "cmd_import",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_import(args):\n    \"\"\"Import fragment collection\"\"\"\n    import_file = Path(args.file)\n    if not import_file.exists():\n        print(f\"Error: Import file not found: {import_file}\")\n        return 1\n    \n    with open(import_file, 'r') as f:\n        data = json.load(f)\n    \n    print(f\"Import: {data.get('name', 'Unknown')}\")\n    print(f\"Description: {data.get('description', 'N/A')}\")\n    \n    metadata = data.get('metadata', {})\n    print(f\"\\nMetadata:\")\n    print(f\"  Fragments: {metadata.get('total', 0)}\")\n    print(f\"  Avg Quality: {metadata.get('avg_quality', 0):.1f}/10\")\n    print(f\"  Languages: {', '.join(metadata.get('languages', []))}\")\n    \n    if args.verbose and 'fragments' in data:\n        print(f\"\\nFragments:\")\n        for i, frag_data in enumerate(data['fragments'][:20], 1):\n            print(f\"  {i}. {frag_data['name']} ({frag_data['type']}) - Quality: {frag_data['quality']}/10\")\n    \n    return 0",
      "lines": 25,
      "quality": 9,
      "start_line": 172,
      "end_line": 196,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 5,
      "documentation_ratio": 0.05,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "ec3c727e41bf1137469335da3bdb37ab",
      "similar_fragments": []
    },
    {
      "uid": "7e54165bc9b99a9e",
      "name": "__post_init__",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "    def __post_init__(self):\n        \"\"\"Set defaults for None values\"\"\"\n        if self.exts is None:\n            self.exts = [\n                \".py\", \".js\", \".jsx\", \".ts\", \".tsx\",\n                \".go\", \".rs\", \".java\", \".c\", \".cpp\", \".h\", \".hpp\",\n                \".rb\", \".php\", \".swift\", \".kt\", \".scala\"\n            ]\n        if self.ignore is None:\n            self.ignore = [\n                \".git\", \"node_modules\", \"__pycache__\", \"venv\", \".venv\",\n                \"env\", \"build\", \"dist\", \".idea\", \".vscode\", \"target\",\n                \"vendor\", \".next\", \".nuxt\", \"coverage\"\n            ]",
      "lines": 14,
      "quality": 9,
      "start_line": 49,
      "end_line": 62,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 4,
      "documentation_ratio": 0.07142857142857142,
      "has_types": false,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "9b48618fb37ba773a7de03153dd0fbe1",
      "similar_fragments": []
    },
    {
      "uid": "edadc818b35cb0e1",
      "name": "format_size",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def format_size(bytes_size: int) -> str:\n    \"\"\"Format byte size to human readable\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if bytes_size < 1024.0:\n            return f\"{bytes_size:.1f} {unit}\"\n        bytes_size /= 1024.0\n    return f\"{bytes_size:.1f} TB\"",
      "lines": 7,
      "quality": 9,
      "start_line": 38,
      "end_line": 44,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 3,
      "documentation_ratio": 0.14285714285714285,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "c7de4934f41074c8c00767acf7da999d",
      "similar_fragments": []
    },
    {
      "uid": "dd984825804f277c",
      "name": "stable_uid",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def stable_uid(project: str, filename: str, line: str) -> str:\n    \"\"\"\n    Generate stable unique identifier for a fragment\n    \"\"\"\n    key = f\"{project}:{filename}:{line}\"\n    return hashlib.md5(key.encode()).hexdigest()[:16]",
      "lines": 6,
      "quality": 9,
      "start_line": 30,
      "end_line": 35,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.3333333333333333,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "99733b4db2667388a82524112705de11",
      "similar_fragments": []
    },
    {
      "uid": "505ffea4cac588f7",
      "name": "to_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization\"\"\"\n        data = asdict(self)\n        data['languages'] = list(self.languages)  # Convert set to list\n        return data",
      "lines": 5,
      "quality": 9,
      "start_line": 42,
      "end_line": 46,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "57256fad1b9281c320844a5639427ff9",
      "similar_fragments": []
    },
    {
      "uid": "737850a9e48693e7",
      "name": "ensure_dirs",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "def ensure_dirs():\n    \"\"\"Ensure required directories exist\"\"\"\n    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n    CACHE_DIR.mkdir(exist_ok=True)\n    EXPORT_DIR.mkdir(exist_ok=True)",
      "lines": 5,
      "quality": 9,
      "start_line": 95,
      "end_line": 99,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 1,
      "documentation_ratio": 0.2,
      "has_types": false,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "e57dc4ba4fe7b99416af42ed325c1540",
      "similar_fragments": []
    },
    {
      "uid": "5e53e03506de5df5",
      "name": "truncate_string",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/utils.py",
      "project": "autopsy_pro_v3",
      "code": "def truncate_string(s: str, max_len: int = 100, suffix: str = \"...\") -> str:\n    \"\"\"Truncate string to max length\"\"\"\n    if len(s) <= max_len:\n        return s\n    return s[:max_len - len(suffix)] + suffix",
      "lines": 5,
      "quality": 9,
      "start_line": 73,
      "end_line": 77,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.2,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "d3ff14694966f7b6374b1d7df428ec3f",
      "similar_fragments": []
    },
    {
      "uid": "9c369bdb3d94c2e1",
      "name": "cmd_extract",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_extract(args):\n    \"\"\"Extract code fragments\"\"\"\n    config = load_config()\n    \n    # Override config\n    if args.min_quality is not None:\n        config.min_quality = args.min_quality\n    if args.skip_tests:\n        config.skip_tests = True\n    if args.no_dedupe:\n        config.deduplicate = False\n    \n    # Load projects\n    if args.scan_file:\n        scan_file = Path(args.scan_file)\n        if not scan_file.exists():\n            print(f\"Error: Scan file not found: {scan_file}\")\n            return 1\n        \n        with open(scan_file, 'r') as f:\n            scan_data = json.load(f)\n        \n        from .models import ScanResult\n        scan_result = ScanResult.from_dict(scan_data)\n        projects = scan_result.projects\n    else:\n        # Need to scan first\n        base_path = Path(args.directory).expanduser().resolve()\n        print(f\"Scanning {base_path}...\")\n        scan_result = scan_projects(base_path, config)\n        projects = scan_result.projects\n        print(f\"Found {len(projects)} projects\")\n    \n    print(f\"Extracting fragments...\")\n    result = extract_fragments(projects, config)\n    \n    print(f\"\\nExtraction Results:\")\n    print(f\"  Fragments extracted: {len(result.fragments)}\")\n    print(f\"  Total lines: {result.total_lines:,}\")\n    print(f\"  Average quality: {result.avg_quality:.1f}/10\")\n    print(f\"  Languages: {', '.join(sorted(result.languages))}\")\n    print(f\"  Extraction time: {result.extraction_time:.2f}s\")\n    \n    if args.verbose:\n        # Show top fragments\n        print(f\"\\nTop 20 fragments by quality:\")\n        for i, f in enumerate(result.fragments[:20], 1):\n            print(f\"  {i}. {f.name} ({f.type}) - Quality: {f.quality}/10, Lines: {f.lines}\")\n    \n    # Save if requested\n    if args.output:\n        output_path = Path(args.output)\n        with open(output_path, 'w') as f:\n            json.dump(result.to_dict(), f, indent=2)\n        print(f\"\\nResults saved to: {output_path}\")\n    \n    return 0",
      "lines": 57,
      "quality": 8,
      "start_line": 65,
      "end_line": 121,
      "dependencies": [],
      "imports": [
        "models"
      ],
      "exports": [],
      "complexity": 11,
      "documentation_ratio": 0.125,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "c69c1cf2faedc3b2562e7a7a44878345",
      "similar_fragments": []
    },
    {
      "uid": "7dc0a12e63af7fc5",
      "name": "cmd_export",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_export(args):\n    \"\"\"Export fragment collection\"\"\"\n    # Load fragments\n    if not args.fragments_file:\n        print(\"Error: --fragments-file required for export\")\n        return 1\n    \n    fragments_file = Path(args.fragments_file)\n    if not fragments_file.exists():\n        print(f\"Error: Fragments file not found: {fragments_file}\")\n        return 1\n    \n    with open(fragments_file, 'r') as f:\n        data = json.load(f)\n    \n    from .models import ExtractionResult\n    result = ExtractionResult.from_dict(data)\n    \n    # Filter if requested\n    fragments = result.fragments\n    if args.min_quality:\n        fragments = [f for f in fragments if f.quality >= args.min_quality]\n    if args.language:\n        fragments = [f for f in fragments if args.language.lower() in f.language.lower()]\n    \n    print(f\"Exporting {len(fragments)} fragments...\")\n    \n    # Create export\n    export_data = {\n        'name': args.name or 'export',\n        'description': args.description or 'Exported fragment collection',\n        'fragments': [f.to_dict() for f in fragments],\n        'metadata': {\n            'total': len(fragments),\n            'avg_quality': sum(f.quality for f in fragments) / len(fragments) if fragments else 0,\n            'languages': list({f.language for f in fragments})\n        }\n    }\n    \n    # Save\n    export_path = get_export_path(args.name or 'export')\n    with open(export_path, 'w') as f:\n        json.dump(export_data, f, indent=2)\n    \n    print(f\"Export saved to: {export_path}\")\n    return 0",
      "lines": 46,
      "quality": 8,
      "start_line": 124,
      "end_line": 169,
      "dependencies": [],
      "imports": [
        "models"
      ],
      "exports": [],
      "complexity": 19,
      "documentation_ratio": 0.13157894736842105,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "497618fc3847773d3fb7622e11ebff57",
      "similar_fragments": []
    },
    {
      "uid": "ab19e901b59b304f",
      "name": "extract_fragments_parallel",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_fragments_parallel(projects: List[Project], config: Config) -> List[Fragment]:\n    \"\"\"\n    Extract fragments from projects using parallel processing\n    \"\"\"\n    all_fragments = []\n    total_files = sum(len(p.code_files) for p in projects)\n    \n    logger.info(f\"Extracting fragments from {total_files} files across {len(projects)} projects\")\n    \n    if config.parallel_scan and total_files > 10:\n        # Parallel extraction\n        with ThreadPoolExecutor(max_workers=config.max_workers) as executor:\n            futures = []\n            for project in projects:\n                for file_path in project.code_files:\n                    future = executor.submit(extract_from_file, Path(file_path), project, config)\n                    futures.append(future)\n            \n            for future in as_completed(futures):\n                try:\n                    fragments = future.result()\n                    all_fragments.extend(fragments)\n                except Exception as e:\n                    logger.error(f\"Extraction error: {e}\")\n    else:\n        # Sequential extraction\n        for project in projects:\n            for file_path in project.code_files:\n                fragments = extract_from_file(Path(file_path), project, config)\n                all_fragments.extend(fragments)\n    \n    logger.info(f\"Extracted {len(all_fragments)} raw fragments\")\n    \n    # Deduplicate if configured\n    if config.deduplicate:\n        all_fragments = deduplicate_fragments(all_fragments)\n    \n    # Filter tests if configured\n    if config.skip_tests:\n        original_count = len(all_fragments)\n        all_fragments = [f for f in all_fragments if not f.has_tests]\n        logger.info(f\"Filtered out {original_count - len(all_fragments)} test fragments\")\n    \n    return all_fragments",
      "lines": 44,
      "quality": 8,
      "start_line": 428,
      "end_line": 471,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 17,
      "documentation_ratio": 0.16216216216216217,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "75d316ead873cf5ddcab9f29c3e88beb",
      "similar_fragments": []
    },
    {
      "uid": "a23f0cd24740533c",
      "name": "cmd_config",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_config(args):\n    \"\"\"Manage configuration\"\"\"\n    if args.show:\n        config = load_config()\n        print(\"Current Configuration:\")\n        print(json.dumps(config.to_dict(), indent=2))\n    elif args.reset:\n        config = Config()\n        save_config(config)\n        print(\"Configuration reset to defaults\")\n    elif args.set:\n        config = load_config()\n        key, value = args.set.split('=', 1)\n        \n        # Parse value\n        if value.lower() in ['true', 'false']:\n            value = value.lower() == 'true'\n        elif value.isdigit():\n            value = int(value)\n        elif '.' in value and value.replace('.', '').isdigit():\n            value = float(value)\n        \n        # Set value\n        if hasattr(config, key):\n            setattr(config, key, value)\n            if save_config(config):\n                print(f\"Set {key} = {value}\")\n            else:\n                print(f\"Error: Could not save configuration\")\n                return 1\n        else:\n            print(f\"Error: Unknown config key: {key}\")\n            return 1\n    \n    return 0",
      "lines": 35,
      "quality": 8,
      "start_line": 199,
      "end_line": 233,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 12,
      "documentation_ratio": 0.09375,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "e1eeede148f8538814fda56bf5a08862",
      "similar_fragments": []
    },
    {
      "uid": "db579894e442325f",
      "name": "to_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'base_path': self.base_path,\n            'projects': [p.to_dict() for p in self.projects],\n            'scan_time': self.scan_time,\n            'timestamp': self.timestamp.isoformat()\n        }",
      "lines": 8,
      "quality": 8,
      "start_line": 158,
      "end_line": 165,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.125,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "6ca95d1c3321a67443c7ab10486a9191",
      "similar_fragments": []
    },
    {
      "uid": "d935a1d448b342d6",
      "name": "from_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def from_dict(cls, data: Dict[str, Any]) -> 'ScanResult':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            base_path=data['base_path'],\n            projects=[Project.from_dict(p) for p in data['projects']],\n            scan_time=data['scan_time'],\n            timestamp=datetime.fromisoformat(data['timestamp'])\n        )",
      "lines": 8,
      "quality": 8,
      "start_line": 168,
      "end_line": 175,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.125,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "9fa995d3825c88aef389955098176d55",
      "similar_fragments": []
    },
    {
      "uid": "9ca84f5f743689dd",
      "name": "to_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return {\n            'fragments': [f.to_dict() for f in self.fragments],\n            'extraction_time': self.extraction_time,\n            'timestamp': self.timestamp.isoformat()\n        }",
      "lines": 7,
      "quality": 8,
      "start_line": 199,
      "end_line": 205,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.14285714285714285,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "10d20113341d25dccc2a277df6c6eff5",
      "similar_fragments": []
    },
    {
      "uid": "f57cf0bf3f8b333d",
      "name": "from_dict",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def from_dict(cls, data: Dict[str, Any]) -> 'ExtractionResult':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(\n            fragments=[Fragment.from_dict(f) for f in data['fragments']],\n            extraction_time=data['extraction_time'],\n            timestamp=datetime.fromisoformat(data['timestamp'])\n        )",
      "lines": 7,
      "quality": 8,
      "start_line": 208,
      "end_line": 214,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.14285714285714285,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "a0fffd665cbd6688eb031c6b4fdd146c",
      "similar_fragments": []
    },
    {
      "uid": "24b8f0edd0508b5b",
      "name": "assess_quality_enhanced",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def assess_quality_enhanced(code: str, lang: str, fragment_type: str) -> Tuple[int, Dict[str, any]]:\n    \"\"\"\n    Enhanced quality assessment with detailed metrics\n    Returns: (quality_score 1-10, metrics_dict)\n    \"\"\"\n    lines = code.splitlines()\n    text = code.lower()\n    score = 5  # Start neutral\n    metrics = {}\n    \n    # Line count scoring\n    line_count = len([l for l in lines if l.strip()])\n    metrics['line_count'] = line_count\n    \n    if 10 <= line_count <= 50:\n        score += 2\n        metrics['length_score'] = 2\n    elif 5 <= line_count < 10 or 50 < line_count <= 100:\n        score += 1\n        metrics['length_score'] = 1\n    elif line_count > 200:\n        score -= 2\n        metrics['length_score'] = -2\n    else:\n        metrics['length_score'] = 0\n    \n    # Documentation ratio\n    doc_ratio = compute_documentation_ratio(code, lang)\n    metrics['doc_ratio'] = doc_ratio\n    \n    if doc_ratio > 0.15:\n        score += 2\n    elif doc_ratio > 0.05:\n        score += 1\n    \n    # Type hints (Python)\n    if lang in ['py', 'python']:\n        has_types = '->' in code or ': ' in code\n        metrics['has_types'] = has_types\n        if has_types:\n            score += 1\n    \n    # TypeScript/typed code\n    if lang in ['typescript', 'ts']:\n        has_types = ': ' in code or 'interface' in text or 'type ' in text\n        metrics['has_types'] = has_types\n        if has_types:\n            score += 1\n    \n    # Async patterns\n    if 'async' in text or 'await' in text:\n        score += 1\n        metrics['has_async'] = True\n    \n    # Error handling\n    has_error_handling = any(kw in text for kw in ['try', 'catch', 'except', 'error', 'throw'])\n    metrics['has_error_handling'] = has_error_handling\n    if has_error_handling:\n        score += 1\n    \n    # Export declarations (good for reusability)\n    if 'export' in text:\n        score += 1\n        metrics['has_exports'] = True\n    \n    # Negative factors\n    \n    # TODO/FIXME markers\n    has_todos = 'todo' in text or 'fixme' in text or 'hack' in text\n    metrics['has_todos'] = has_todos\n    if has_todos:\n        score -= 1\n    \n    # Debug statements\n    debug_patterns = ['console.log', 'print(', 'println', 'fmt.println', 'debugger']\n    has_debug = any(pat in text for pat in debug_patterns)\n    metrics['has_debug'] = has_debug\n    if has_debug:\n        score -= 1\n    \n    # Tests (depending on context, might be positive or filtered separately)\n    is_test = 'test' in text or 'spec' in text or 'describe' in text\n    metrics['is_test'] = is_test\n    if is_test and 'test' in fragment_type.lower():\n        score += 1  # Good if we're looking for tests\n    \n    # Complexity\n    complexity = compute_complexity(code, lang)\n    metrics['complexity'] = complexity\n    \n    if complexity > 15:\n        score -= 2\n    elif complexity > 10:\n        score -= 1\n    elif 3 <= complexity <= 8:\n        score += 1\n    \n    # Nesting depth\n    max_indent = 0\n    for line in lines:\n        if line.strip():\n            indent = len(line) - len(line.lstrip())\n            max_indent = max(max_indent, indent)\n    \n    metrics['max_indent'] = max_indent\n    if max_indent > 24:\n        score -= 2\n    elif max_indent > 16:\n        score -= 1\n    \n    # Code smells\n    if code.count('if') > 10:\n        score -= 1\n        metrics['too_many_ifs'] = True\n    \n    final_score = max(1, min(10, score))\n    metrics['final_score'] = final_score\n    \n    return final_score, metrics",
      "lines": 119,
      "quality": 7,
      "start_line": 77,
      "end_line": 195,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 45,
      "documentation_ratio": 0.16161616161616163,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "5744bb23cbec4cfbadf419d7c713f74e",
      "similar_fragments": []
    },
    {
      "uid": "c3e085607d49b1ae",
      "name": "Fragment",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "class Fragment:\n    \"\"\"Code fragment with enhanced metadata\"\"\"\n    uid: str\n    name: str\n    type: str\n    file: str\n    project: str\n    code: str\n    lines: int\n    quality: int\n    start_line: int\n    \n    # Enhanced metadata\n    end_line: int = 0\n    dependencies: List[str] = field(default_factory=list)\n    imports: List[str] = field(default_factory=list)\n    exports: List[str] = field(default_factory=list)\n    complexity: int = 0\n    documentation_ratio: float = 0.0\n    has_types: bool = False\n    has_tests: bool = False\n    has_error_handling: bool = False\n    tags: List[str] = field(default_factory=list)\n    \n    # Semantic analysis\n    embedding_hash: Optional[str] = None\n    similar_fragments: List[str] = field(default_factory=list)\n    \n    @property\n    def file_name(self) -> str:\n        \"\"\"Just the filename\"\"\"\n        return Path(self.file).name\n    \n    @property\n    def language(self) -> str:\n        \"\"\"Infer language from type\"\"\"\n        type_lower = self.type.lower()\n        if 'python' in type_lower:\n            return 'Python'\n        elif 'js' in type_lower or 'typescript' in type_lower or 'react' in type_lower:\n            return 'JavaScript/TypeScript'\n        elif 'go' in type_lower:\n            return 'Go'\n        elif 'rust' in type_lower:\n            return 'Rust'\n        elif 'java' in type_lower:\n            return 'Java'\n        elif 'ruby' in type_lower:\n            return 'Ruby'\n        elif 'php' in type_lower:\n            return 'PHP'\n        elif 'swift' in type_lower:\n            return 'Swift'\n        return 'Unknown'\n    \n    @property\n    def code_hash(self) -> str:\n        \"\"\"Hash of the code for comparison\"\"\"\n        return hashlib.md5(self.code.encode()).hexdigest()\n    \n    def compute_embedding_hash(self) -> str:\n        \"\"\"Compute a simple semantic hash (would use embeddings in production)\"\"\"\n        # Normalize code for semantic comparison\n        normalized = self.code.lower()\n        normalized = ''.join(c if c.isalnum() else ' ' for c in normalized)\n        words = normalized.split()\n        word_set = sorted(set(words))\n        semantic_text = ' '.join(word_set[:50])  # First 50 unique words\n        self.embedding_hash = hashlib.md5(semantic_text.encode()).hexdigest()\n        return self.embedding_hash\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization\"\"\"\n        return asdict(self)\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Fragment':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(**data)",
      "lines": 79,
      "quality": 7,
      "start_line": 57,
      "end_line": 135,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 17,
      "documentation_ratio": 0.14084507042253522,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "e1bf5e42d2f12cadd956f1ee142264d5",
      "similar_fragments": []
    },
    {
      "uid": "df4b945097b3d359",
      "name": "Config",
      "type": "PythonClass",
      "file": "/home/user/autopsy/autopsy_pro_v3/config.py",
      "project": "autopsy_pro_v3",
      "code": "class Config:\n    \"\"\"Configuration data class with defaults\"\"\"\n    # Scanning\n    exts: List[str] = None\n    ignore: List[str] = None\n    inactive_days: int = 60\n    include_active: bool = False\n    max_file_mb: float = 1.0\n    \n    # Extraction\n    min_quality: int = 5\n    min_lines: int = 5\n    max_lines: int = 500\n    skip_tests: bool = False\n    deduplicate: bool = True\n    \n    # Building\n    organize_by_type: bool = True\n    create_readme: bool = True\n    create_tests: bool = False\n    include_deps: bool = True\n    \n    # UI\n    dark_mode: bool = False\n    auto_save: bool = True\n    \n    # Performance\n    parallel_scan: bool = True\n    max_workers: int = 4\n    enable_cache: bool = True\n    cache_ttl_hours: int = 24\n    \n    def __post_init__(self):\n        \"\"\"Set defaults for None values\"\"\"\n        if self.exts is None:\n            self.exts = [\n                \".py\", \".js\", \".jsx\", \".ts\", \".tsx\",\n                \".go\", \".rs\", \".java\", \".c\", \".cpp\", \".h\", \".hpp\",\n                \".rb\", \".php\", \".swift\", \".kt\", \".scala\"\n            ]\n        if self.ignore is None:\n            self.ignore = [\n                \".git\", \"node_modules\", \"__pycache__\", \"venv\", \".venv\",\n                \"env\", \"build\", \"dist\", \".idea\", \".vscode\", \"target\",\n                \"vendor\", \".next\", \".nuxt\", \"coverage\"\n            ]\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        return asdict(self)\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'Config':\n        \"\"\"Create from dictionary\"\"\"\n        return cls(**{k: v for k, v in data.items() if hasattr(cls, k)})\n    \n    def validate(self) -> List[str]:\n        \"\"\"Validate configuration, return list of issues\"\"\"\n        issues = []\n        \n        if self.inactive_days < 0:\n            issues.append(\"inactive_days must be >= 0\")\n        if self.max_file_mb <= 0:\n            issues.append(\"max_file_mb must be > 0\")\n        if not 1 <= self.min_quality <= 10:\n            issues.append(\"min_quality must be between 1-10\")\n        if self.min_lines < 1:\n            issues.append(\"min_lines must be >= 1\")\n        if self.max_lines < self.min_lines:\n            issues.append(\"max_lines must be >= min_lines\")\n        if self.max_workers < 1:\n            issues.append(\"max_workers must be >= 1\")\n        if self.cache_ttl_hours < 1:\n            issues.append(\"cache_ttl_hours must be >= 1\")\n        \n        return issues",
      "lines": 76,
      "quality": 7,
      "start_line": 17,
      "end_line": 92,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 13,
      "documentation_ratio": 0.15151515151515152,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "6dddcc257ddc6df504c9d84439f82867",
      "similar_fragments": []
    },
    {
      "uid": "04516b91f6534120",
      "name": "extract_js_block",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_js_block(lines: List[str], start_line: int) -> Tuple[str, int]:\n    \"\"\"Extract JavaScript/TypeScript block with brace matching\"\"\"\n    if start_line >= len(lines):\n        return \"\", start_line\n    \n    line = lines[start_line]\n    brace = 0\n    seen_open = False\n    \n    for ch in line:\n        if ch == \"{\":\n            brace += 1\n            seen_open = True\n        elif ch == \"}\":\n            brace -= 1\n    \n    if not seen_open:\n        return line, start_line\n    \n    block = [line]\n    end_line = start_line\n    \n    for i in range(start_line + 1, min(len(lines), start_line + 500)):  # Limit search\n        end_line = i\n        block.append(lines[i])\n        \n        for ch in lines[i]:\n            if ch == \"{\":\n                brace += 1\n            elif ch == \"}\":\n                brace -= 1\n        \n        if brace <= 0:\n            break\n    \n    return \"\\n\".join(block), end_line",
      "lines": 36,
      "quality": 7,
      "start_line": 275,
      "end_line": 310,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 11,
      "documentation_ratio": 0.03571428571428571,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "452da93714eeed2263a47f423c320a22",
      "similar_fragments": []
    },
    {
      "uid": "d4bacd73abdd1515",
      "name": "compute_documentation_ratio",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def compute_documentation_ratio(code: str, lang: str) -> float:\n    \"\"\"\n    Compute ratio of documentation to code\n    \"\"\"\n    lines = code.splitlines()\n    doc_lines = 0\n    code_lines = 0\n    \n    for line in lines:\n        stripped = line.strip()\n        if not stripped:\n            continue\n        \n        code_lines += 1\n        \n        # Check for comments\n        if lang in ['py', 'python']:\n            if stripped.startswith('#') or stripped.startswith('\"\"\"') or stripped.startswith(\"'''\"):\n                doc_lines += 1\n        elif lang in ['js', 'javascript', 'typescript', 'go', 'rust', 'java', 'c', 'cpp']:\n            if stripped.startswith('//') or stripped.startswith('/*') or stripped.startswith('*'):\n                doc_lines += 1\n        elif lang in ['ruby']:\n            if stripped.startswith('#'):\n                doc_lines += 1\n        elif lang in ['php']:\n            if stripped.startswith('//') or stripped.startswith('#') or stripped.startswith('/*'):\n                doc_lines += 1\n    \n    return doc_lines / code_lines if code_lines > 0 else 0.0",
      "lines": 30,
      "quality": 7,
      "start_line": 45,
      "end_line": 74,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 20,
      "documentation_ratio": 0.11538461538461539,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "4bbc241b120e8c617b4054a77b7cf3c5",
      "similar_fragments": []
    },
    {
      "uid": "4958c8e36a76e1d8",
      "name": "language",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/models.py",
      "project": "autopsy_pro_v3",
      "code": "    def language(self) -> str:\n        \"\"\"Infer language from type\"\"\"\n        type_lower = self.type.lower()\n        if 'python' in type_lower:\n            return 'Python'\n        elif 'js' in type_lower or 'typescript' in type_lower or 'react' in type_lower:\n            return 'JavaScript/TypeScript'\n        elif 'go' in type_lower:\n            return 'Go'\n        elif 'rust' in type_lower:\n            return 'Rust'\n        elif 'java' in type_lower:\n            return 'Java'\n        elif 'ruby' in type_lower:\n            return 'Ruby'\n        elif 'php' in type_lower:\n            return 'PHP'\n        elif 'swift' in type_lower:\n            return 'Swift'\n        return 'Unknown'",
      "lines": 20,
      "quality": 7,
      "start_line": 91,
      "end_line": 110,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 11,
      "documentation_ratio": 0.05,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "63ed3008c0889754c42a973eb7fcf3d4",
      "similar_fragments": []
    },
    {
      "uid": "67cc1f3c2005a762",
      "name": "cmd_cache",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/cli.py",
      "project": "autopsy_pro_v3",
      "code": "def cmd_cache(args):\n    \"\"\"Manage cache\"\"\"\n    if args.clear:\n        clear_cache()\n        print(\"Cache cleared\")\n    \n    return 0",
      "lines": 7,
      "quality": 7,
      "start_line": 236,
      "end_line": 242,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 2,
      "documentation_ratio": 0.16666666666666666,
      "has_types": false,
      "has_tests": false,
      "has_error_handling": false,
      "tags": [],
      "embedding_hash": "c3bcaf087c5e82ac9d019036e3c892f8",
      "similar_fragments": []
    },
    {
      "uid": "e4150adc41e2622a",
      "name": "scan_projects_parallel",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/scanner.py",
      "project": "autopsy_pro_v3",
      "code": "def scan_projects_parallel(base: Path, config: Config) -> List[Project]:\n    \"\"\"\n    Enhanced parallel project scanner\n    \"\"\"\n    start_time = time.time()\n    project_roots = []\n    \n    logger.info(f\"Scanning {base} for projects...\")\n    logger.info(f\"Extensions: {config.exts}\")\n    logger.info(f\"Ignored: {config.ignore}\")\n    logger.info(f\"Inactive threshold: {config.inactive_days} days\")\n    \n    # First pass: identify potential project roots\n    try:\n        for root, dirs, files in os.walk(base):\n            # Filter ignored directories\n            dirs[:] = [d for d in dirs if d not in config.ignore and not d.startswith('.')]\n            \n            root_path = Path(root)\n            \n            # Skip if we're already inside a detected project\n            skip = False\n            for pr in project_roots:\n                try:\n                    if root_path.is_relative_to(pr) and root_path != pr:\n                        skip = True\n                        break\n                except ValueError:\n                    continue\n            \n            if skip:\n                continue\n            \n            # Check if this is a project root\n            if is_project_root(root_path, files):\n                project_roots.append(root_path)\n                # Don't descend into detected projects\n                dirs.clear()\n    except Exception as e:\n        logger.error(f\"Error during directory walk: {e}\")\n        return []\n    \n    logger.info(f\"Found {len(project_roots)} potential project roots\")\n    \n    # Second pass: process projects in parallel\n    projects = []\n    \n    if config.parallel_scan and len(project_roots) > 1:\n        with ThreadPoolExecutor(max_workers=config.max_workers) as executor:\n            future_to_path = {\n                executor.submit(scan_directory, path, config): path\n                for path in project_roots\n            }\n            \n            for future in as_completed(future_to_path):\n                path = future_to_path[future]\n                try:\n                    project = future.result()\n                    if project:\n                        projects.append(project)\n                except Exception as e:\n                    logger.error(f\"Error scanning {path}: {e}\")\n    else:\n        # Sequential scanning\n        for path in project_roots:\n            project = scan_directory(path, config)\n            if project:\n                projects.append(project)\n    \n    scan_time = time.time() - start_time\n    logger.info(f\"Scan complete: Found {len(projects)} projects in {scan_time:.2f}s\")\n    \n    # Sort by last modified (most recent first)\n    projects.sort(key=lambda p: p.last_modified, reverse=True)\n    \n    return projects",
      "lines": 76,
      "quality": 6,
      "start_line": 258,
      "end_line": 333,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 24,
      "documentation_ratio": 0.15873015873015872,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "c560e222a412ec06af33332c2e306331",
      "similar_fragments": []
    },
    {
      "uid": "f4a430015d72a6a1",
      "name": "extract_python_fragments",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/extractor.py",
      "project": "autopsy_pro_v3",
      "code": "def extract_python_fragments(code: str, file_path: Path, project_name: str, config: Config) -> List[Fragment]:\n    \"\"\"Extract Python functions and classes using AST\"\"\"\n    fragments = []\n    \n    try:\n        tree = ast.parse(code)\n        lines = code.splitlines()\n        \n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                # Get block\n                start = getattr(node, \"lineno\", 1) - 1\n                end = getattr(node, \"end_lineno\", start + 1)\n                \n                if 0 <= start < len(lines) and end <= len(lines):\n                    block_lines = lines[start:end]\n                    block = \"\\n\".join(block_lines)\n                    \n                    # Skip if too short or too long\n                    if len(block_lines) < config.min_lines or len(block_lines) > config.max_lines:\n                        continue\n                    \n                    # Determine type\n                    if isinstance(node, ast.ClassDef):\n                        frag_type = \"PythonClass\"\n                    elif isinstance(node, ast.AsyncFunctionDef):\n                        frag_type = \"PythonAsyncFunc\"\n                    else:\n                        frag_type = \"PythonFunc\"\n                    \n                    # Extract imports (simple approach)\n                    imports = []\n                    for imp_node in ast.walk(node):\n                        if isinstance(imp_node, ast.Import):\n                            imports.extend([alias.name for alias in imp_node.names])\n                        elif isinstance(imp_node, ast.ImportFrom):\n                            if imp_node.module:\n                                imports.append(imp_node.module)\n                    \n                    # Quality assessment\n                    quality, metrics = assess_quality_enhanced(block, 'python', frag_type)\n                    \n                    # Skip low quality if configured\n                    if quality < config.min_quality:\n                        continue\n                    \n                    uid = stable_uid(project_name, file_path.name, str(start))\n                    \n                    fragment = Fragment(\n                        uid=uid,\n                        name=node.name,\n                        type=frag_type,\n                        file=str(file_path),\n                        project=project_name,\n                        code=block,\n                        lines=len(block_lines),\n                        quality=quality,\n                        start_line=start + 1,\n                        end_line=end,\n                        imports=imports,\n                        complexity=metrics.get('complexity', 0),\n                        documentation_ratio=metrics.get('doc_ratio', 0.0),\n                        has_types=metrics.get('has_types', False),\n                        has_error_handling=metrics.get('has_error_handling', False),\n                        has_tests=metrics.get('is_test', False)\n                    )\n                    \n                    fragments.append(fragment)\n    \n    except SyntaxError as e:\n        logger.warning(f\"Syntax error in {file_path}: {e}\")\n    except Exception as e:\n        logger.error(f\"Error extracting Python from {file_path}: {e}\")\n    \n    return fragments",
      "lines": 75,
      "quality": 5,
      "start_line": 198,
      "end_line": 272,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 22,
      "documentation_ratio": 0.11290322580645161,
      "has_types": true,
      "has_tests": true,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "ec03fe302541b60ae8d456ff47f58489",
      "similar_fragments": []
    },
    {
      "uid": "67beb495a149400a",
      "name": "detect_project_info",
      "type": "PythonFunc",
      "file": "/home/user/autopsy/autopsy_pro_v3/scanner.py",
      "project": "autopsy_pro_v3",
      "code": "def detect_project_info(files: List[str], path: Path) -> Tuple[str, List[str], List[str]]:\n    \"\"\"\n    Detect project type, frameworks, and dependencies\n    Returns: (project_type, frameworks, dependencies)\n    \"\"\"\n    project_type = \"Unknown\"\n    frameworks = []\n    dependencies = []\n    \n    # Check indicator files\n    for file in files:\n        if file in PROJECT_INDICATORS:\n            detected_type, deps = PROJECT_INDICATORS[file]\n            if project_type == \"Unknown\":\n                project_type = detected_type\n            frameworks.append(detected_type)\n            dependencies.extend(deps)\n    \n    # Parse package.json for more details\n    if \"package.json\" in files:\n        try:\n            pkg_file = path / \"package.json\"\n            with open(pkg_file, 'r', encoding='utf-8') as f:\n                pkg = json.load(f)\n                deps = pkg.get('dependencies', {})\n                dev_deps = pkg.get('devDependencies', {})\n                \n                # Detect frameworks\n                if 'react' in deps or 'react' in dev_deps:\n                    frameworks.append('React')\n                if 'vue' in deps:\n                    frameworks.append('Vue')\n                if 'express' in deps:\n                    frameworks.append('Express')\n                if 'fastify' in deps:\n                    frameworks.append('Fastify')\n                if '@nestjs/core' in deps:\n                    frameworks.append('NestJS')\n                \n                # Add dependencies\n                dependencies.extend(list(deps.keys())[:10])  # First 10 deps\n        except Exception as e:\n            logger.debug(f\"Error parsing package.json: {e}\")\n    \n    # Fallback detection based on file extensions\n    if project_type == \"Unknown\":\n        extensions = {f.split('.')[-1] for f in files if '.' in f}\n        \n        ext_map = {\n            'py': 'Python',\n            'js': 'JavaScript',\n            'ts': 'TypeScript',\n            'jsx': 'React',\n            'tsx': 'React TypeScript',\n            'go': 'Go',\n            'rs': 'Rust',\n            'java': 'Java',\n            'rb': 'Ruby',\n            'php': 'PHP',\n            'swift': 'Swift',\n            'kt': 'Kotlin',\n            'scala': 'Scala',\n            'c': 'C',\n            'cpp': 'C++',\n            'cs': 'C#',\n        }\n        \n        for ext, lang in ext_map.items():\n            if ext in extensions:\n                project_type = lang\n                break\n    \n    return project_type, list(set(frameworks)), list(set(dependencies))",
      "lines": 73,
      "quality": 5,
      "start_line": 48,
      "end_line": 120,
      "dependencies": [],
      "imports": [],
      "exports": [],
      "complexity": 19,
      "documentation_ratio": 0.1076923076923077,
      "has_types": true,
      "has_tests": false,
      "has_error_handling": true,
      "tags": [],
      "embedding_hash": "dc19b757167606e3db976a937cc034c7",
      "similar_fragments": []
    }
  ],
  "extraction_time": 0.040657997131347656,
  "timestamp": "2025-11-16T15:54:20.020114"
}